{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import TFDistilBertModel"
      ],
      "metadata": {
        "id": "Pt2jbxfjvy9u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "GpfcJkyvv_vK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Already Generated by GEMINI (see intentGenerator.py)"
      ],
      "metadata": {
        "id": "QXU6JUlXwCIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(file_path):\n",
        "    \"\"\"Load and prepare the dataset.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['label'] = label_encoder.fit_transform(df['intent'])\n",
        "\n",
        "    # Check class distribution\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(df['intent'].value_counts())\n",
        "\n",
        "    return df, label_encoder\n",
        "\n",
        "def create_tf_dataset(texts, labels, tokenizer, batch_size=16, is_training=True):\n",
        "    \"\"\"Create TensorFlow dataset.\"\"\"\n",
        "    # Tokenize texts\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {key: val for key, val in encodings.items()},\n",
        "        labels\n",
        "    ))\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "_FeznYYjwGuj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/jakarta_transport_intents_7354_20241214_021252.json\"\n",
        "df, label_encoder = load_and_prepare_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw398l_hxN5y",
        "outputId": "2e735479-c108-4284-cc3e-ae65a7f5bb53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution:\n",
            "intent\n",
            "asking_for_direction      2745\n",
            "service_recommendation    2485\n",
            "analyzing_surroundings    2124\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NkmBdLJWxOoX",
        "outputId": "23bd5606-b242-4da0-8a0c-adae71f11ace"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 intent                                               text  \\\n",
              "0  asking_for_direction  What's the best way to get to the Gambir Stati...   \n",
              "1  asking_for_direction  Can you tell me how to reach the MRT station a...   \n",
              "2  asking_for_direction  I need directions to the nearest KRL station f...   \n",
              "3  asking_for_direction  Could you guide me on how to get to the TransJ...   \n",
              "4  asking_for_direction  I'm at the Dukuh Atas Station. How do I reach ...   \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      1  \n",
              "2      1  \n",
              "3      1  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-016fc571-cefc-4ca0-a8ae-2281905acd4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intent</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asking_for_direction</td>\n",
              "      <td>What's the best way to get to the Gambir Stati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asking_for_direction</td>\n",
              "      <td>Can you tell me how to reach the MRT station a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asking_for_direction</td>\n",
              "      <td>I need directions to the nearest KRL station f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking_for_direction</td>\n",
              "      <td>Could you guide me on how to get to the TransJ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>asking_for_direction</td>\n",
              "      <td>I'm at the Dukuh Atas Station. How do I reach ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-016fc571-cefc-4ca0-a8ae-2281905acd4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-016fc571-cefc-4ca0-a8ae-2281905acd4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-016fc571-cefc-4ca0-a8ae-2281905acd4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-18d27dab-3f68-42ba-b931-cb6b9f5e4387\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-18d27dab-3f68-42ba-b931-cb6b9f5e4387')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-18d27dab-3f68-42ba-b931-cb6b9f5e4387 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7354,\n  \"fields\": [\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"asking_for_direction\",\n          \"analyzing_surroundings\",\n          \"service_recommendation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7354,\n        \"samples\": [\n          \"What is the most accessible route to the Museum Nasional from the Tanah Abang area, considering my visual impairment?\",\n          \"What's the most accessible route to the Taman Mini Indonesia Indah from the Sudirman Station?\",\n          \"I'm at the Blok M bus stop. Could you guide me to the nearest TransJakarta shelter?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LcuZquzFwpBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(texts, labels, tokenizer, batch_size=16, is_training=True):\n",
        "    \"\"\"Create TensorFlow dataset.\"\"\"\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        dict(encodings),\n",
        "        labels\n",
        "    ))\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    if is_training:\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ciLB90ojzDy1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate"
      ],
      "metadata": {
        "id": "NxVL8CS9wts0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillbertIntentModel(tf.keras.Model):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = TFDistilBertModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "\n",
        "        # Make BERT trainable\n",
        "        self.bert.trainable = True\n",
        "\n",
        "        # Additional layers\n",
        "        self.dense1 = tf.keras.layers.Dense(256, activation='gelu')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
        "        self.dense2 = tf.keras.layers.Dense(128, activation='gelu')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        outputs = self.bert(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        pooled_output = tf.reduce_mean(hidden_states, axis=1)\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.dense1(pooled_output)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "5x3_m06VHQI0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(conf_matrix, labels, fold):\n",
        "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Confusion Matrix - Fold {fold+1}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f'confusion_matrix_fold_{fold+1}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_history(history, fold):\n",
        "    \"\"\"Plot training history.\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Loss - Fold {fold+1}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Accuracy - Fold {fold+1}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_history_fold_{fold+1}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "9pEgb9WbzNv3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_metrics(all_metrics):\n",
        "    \"\"\"Calculate average metrics across folds safely.\"\"\"\n",
        "    avg_metrics = {}\n",
        "\n",
        "    try:\n",
        "        # For each metric type we want to average\n",
        "        for metric in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "            if metric == 'accuracy':\n",
        "                scores = [fold_metrics[metric] for fold_metrics in all_metrics]\n",
        "            else:\n",
        "                scores = [fold_metrics[metric]['f1-score'] for fold_metrics in all_metrics]\n",
        "            avg_metrics[metric] = np.mean(scores)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating averages: {e}\")\n",
        "        print(\"Raw metrics:\", all_metrics)\n",
        "        return {}\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def train_and_evaluate(df, label_encoder, model_name=\"distilbert-base-uncased\", n_splits=5):\n",
        "    \"\"\"Updated training pipeline with fixed metrics calculation.\"\"\"\n",
        "    print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    training_args = {\n",
        "        'epochs': 10,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 1e-5,\n",
        "        'weight_decay': 0.01\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    all_metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df['text'], df['label'])):\n",
        "        print(f\"\\nTraining Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "        train_texts = df['text'].iloc[train_idx].tolist()\n",
        "        train_labels = df['label'].iloc[train_idx].tolist()\n",
        "        val_texts = df['text'].iloc[val_idx].tolist()\n",
        "        val_labels = df['label'].iloc[val_idx].tolist()\n",
        "\n",
        "        train_encodings = tokenizer(\n",
        "            train_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"tf\"\n",
        "        )\n",
        "\n",
        "        val_encodings = tokenizer(\n",
        "            val_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"tf\"\n",
        "        )\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(train_encodings),\n",
        "            train_labels\n",
        "        )).shuffle(1000).batch(training_args['batch_size'])\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(val_encodings),\n",
        "            val_labels\n",
        "        )).batch(training_args['batch_size'])\n",
        "\n",
        "        model = DistillbertIntentModel(model_name, len(label_encoder.classes_))\n",
        "\n",
        "        optimizer = tf.keras.optimizers.AdamW(\n",
        "            learning_rate=training_args['learning_rate'],\n",
        "            weight_decay=training_args['weight_decay']\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=3,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=2,\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=training_args['epochs'],\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        predictions = model.predict(val_dataset)\n",
        "        pred_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        pred_labels = label_encoder.inverse_transform(pred_labels)\n",
        "        true_labels = label_encoder.inverse_transform(val_labels)\n",
        "\n",
        "        try:\n",
        "            fold_report = classification_report(\n",
        "                true_labels,\n",
        "                pred_labels,\n",
        "                output_dict=True,\n",
        "                zero_division=0\n",
        "            )\n",
        "            all_metrics.append(fold_report)\n",
        "\n",
        "            print(f\"\\nFold {fold + 1} Results:\")\n",
        "            print(classification_report(true_labels, pred_labels))\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics for fold {fold + 1}: {e}\")\n",
        "\n",
        "        # Save model\n",
        "        model.save_weights(f'best_model_fold_{fold+1}')\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    print(\"\\nAverage Metrics Across All Folds:\")\n",
        "    avg_metrics = calculate_average_metrics(all_metrics)\n",
        "\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return all_metrics, avg_metrics"
      ],
      "metadata": {
        "id": "Dl_WLoneHEqM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8WZNhMtvmcE",
        "outputId": "5e004984-5438-4d2f-eb5b-159025ed80a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "368/368 [==============================] - 60s 117ms/step - loss: 0.1710 - accuracy: 0.9419 - val_loss: 0.0663 - val_accuracy: 0.9728 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "368/368 [==============================] - 39s 107ms/step - loss: 0.0547 - accuracy: 0.9808 - val_loss: 0.0703 - val_accuracy: 0.9755 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0479 - accuracy: 0.9815 - val_loss: 0.0671 - val_accuracy: 0.9735 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "368/368 [==============================] - 39s 105ms/step - loss: 0.0312 - accuracy: 0.9891 - val_loss: 0.0594 - val_accuracy: 0.9776 - lr: 5.0000e-06\n",
            "Epoch 5/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0255 - accuracy: 0.9907 - val_loss: 0.0613 - val_accuracy: 0.9803 - lr: 5.0000e-06\n",
            "Epoch 6/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.0765 - val_accuracy: 0.9789 - lr: 5.0000e-06\n",
            "Epoch 7/10\n",
            "368/368 [==============================] - 43s 117ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0671 - val_accuracy: 0.9796 - lr: 2.5000e-06\n",
            "92/92 [==============================] - 43s 34ms/step\n",
            "\n",
            "Fold 1 Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "analyzing_surroundings       1.00      1.00      1.00       425\n",
            "  asking_for_direction       0.97      0.97      0.97       549\n",
            "service_recommendation       0.97      0.97      0.97       497\n",
            "\n",
            "              accuracy                           0.98      1471\n",
            "             macro avg       0.98      0.98      0.98      1471\n",
            "          weighted avg       0.98      0.98      0.98      1471\n",
            "\n",
            "\n",
            "Training Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "368/368 [==============================] - 61s 122ms/step - loss: 0.1861 - accuracy: 0.9308 - val_loss: 0.0507 - val_accuracy: 0.9810 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "368/368 [==============================] - 39s 105ms/step - loss: 0.0607 - accuracy: 0.9782 - val_loss: 0.0579 - val_accuracy: 0.9803 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.0993 - val_accuracy: 0.9633 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0340 - accuracy: 0.9871 - val_loss: 0.0379 - val_accuracy: 0.9871 - lr: 5.0000e-06\n",
            "Epoch 5/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0300 - accuracy: 0.9881 - val_loss: 0.0422 - val_accuracy: 0.9850 - lr: 5.0000e-06\n",
            "Epoch 6/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0223 - accuracy: 0.9915 - val_loss: 0.0624 - val_accuracy: 0.9803 - lr: 5.0000e-06\n",
            "Epoch 7/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 0.0362 - val_accuracy: 0.9878 - lr: 2.5000e-06\n",
            "Epoch 8/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0150 - accuracy: 0.9942 - val_loss: 0.0402 - val_accuracy: 0.9884 - lr: 2.5000e-06\n",
            "Epoch 9/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 0.0406 - val_accuracy: 0.9864 - lr: 2.5000e-06\n",
            "Epoch 10/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9871 - lr: 1.2500e-06\n",
            "92/92 [==============================] - 7s 29ms/step\n",
            "\n",
            "Fold 2 Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "analyzing_surroundings       1.00      1.00      1.00       425\n",
            "  asking_for_direction       0.99      0.98      0.98       549\n",
            "service_recommendation       0.98      0.99      0.98       497\n",
            "\n",
            "              accuracy                           0.99      1471\n",
            "             macro avg       0.99      0.99      0.99      1471\n",
            "          weighted avg       0.99      0.99      0.99      1471\n",
            "\n",
            "\n",
            "Training Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "368/368 [==============================] - 59s 114ms/step - loss: 0.1704 - accuracy: 0.9398 - val_loss: 0.1043 - val_accuracy: 0.9680 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "368/368 [==============================] - 41s 111ms/step - loss: 0.0617 - accuracy: 0.9779 - val_loss: 0.0646 - val_accuracy: 0.9810 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0358 - accuracy: 0.9859 - val_loss: 0.0820 - val_accuracy: 0.9735 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "368/368 [==============================] - 38s 102ms/step - loss: 0.0328 - accuracy: 0.9862 - val_loss: 0.0688 - val_accuracy: 0.9803 - lr: 1.0000e-05\n",
            "Epoch 5/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0703 - val_accuracy: 0.9803 - lr: 5.0000e-06\n",
            "92/92 [==============================] - 7s 26ms/step\n",
            "\n",
            "Fold 3 Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "analyzing_surroundings       1.00      1.00      1.00       425\n",
            "  asking_for_direction       0.97      0.98      0.98       549\n",
            "service_recommendation       0.98      0.96      0.97       497\n",
            "\n",
            "              accuracy                           0.98      1471\n",
            "             macro avg       0.98      0.98      0.98      1471\n",
            "          weighted avg       0.98      0.98      0.98      1471\n",
            "\n",
            "\n",
            "Training Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "368/368 [==============================] - 60s 119ms/step - loss: 0.1764 - accuracy: 0.9346 - val_loss: 0.0828 - val_accuracy: 0.9735 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0612 - accuracy: 0.9793 - val_loss: 0.0696 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0437 - accuracy: 0.9847 - val_loss: 0.0670 - val_accuracy: 0.9755 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0325 - accuracy: 0.9879 - val_loss: 0.0786 - val_accuracy: 0.9755 - lr: 1.0000e-05\n",
            "Epoch 5/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0281 - accuracy: 0.9884 - val_loss: 0.0697 - val_accuracy: 0.9796 - lr: 1.0000e-05\n",
            "Epoch 6/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0174 - accuracy: 0.9934 - val_loss: 0.0603 - val_accuracy: 0.9837 - lr: 5.0000e-06\n",
            "Epoch 7/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0141 - accuracy: 0.9951 - val_loss: 0.0663 - val_accuracy: 0.9844 - lr: 5.0000e-06\n",
            "Epoch 8/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.0742 - val_accuracy: 0.9803 - lr: 5.0000e-06\n",
            "Epoch 9/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 0.0780 - val_accuracy: 0.9844 - lr: 2.5000e-06\n",
            "92/92 [==============================] - 4s 29ms/step\n",
            "\n",
            "Fold 4 Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "analyzing_surroundings       1.00      1.00      1.00       425\n",
            "  asking_for_direction       0.97      0.98      0.98       549\n",
            "service_recommendation       0.98      0.97      0.98       497\n",
            "\n",
            "              accuracy                           0.98      1471\n",
            "             macro avg       0.98      0.98      0.98      1471\n",
            "          weighted avg       0.98      0.98      0.98      1471\n",
            "\n",
            "\n",
            "Training Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "368/368 [==============================] - 59s 114ms/step - loss: 0.1632 - accuracy: 0.9403 - val_loss: 0.0585 - val_accuracy: 0.9748 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "368/368 [==============================] - 38s 104ms/step - loss: 0.0559 - accuracy: 0.9806 - val_loss: 0.0725 - val_accuracy: 0.9803 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0476 - accuracy: 0.9806 - val_loss: 0.0401 - val_accuracy: 0.9789 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0345 - accuracy: 0.9876 - val_loss: 0.0462 - val_accuracy: 0.9796 - lr: 1.0000e-05\n",
            "Epoch 5/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0288 - accuracy: 0.9891 - val_loss: 0.0457 - val_accuracy: 0.9810 - lr: 1.0000e-05\n",
            "Epoch 6/10\n",
            "368/368 [==============================] - 38s 103ms/step - loss: 0.0182 - accuracy: 0.9932 - val_loss: 0.0581 - val_accuracy: 0.9762 - lr: 5.0000e-06\n",
            "92/92 [==============================] - 4s 28ms/step\n",
            "\n",
            "Fold 5 Results:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "analyzing_surroundings       1.00      1.00      1.00       424\n",
            "  asking_for_direction       0.97      0.98      0.97       549\n",
            "service_recommendation       0.97      0.96      0.97       497\n",
            "\n",
            "              accuracy                           0.98      1470\n",
            "             macro avg       0.98      0.98      0.98      1470\n",
            "          weighted avg       0.98      0.98      0.98      1470\n",
            "\n",
            "\n",
            "Average Metrics Across All Folds:\n",
            "accuracy: 0.9818\n",
            "macro avg: 0.9828\n",
            "weighted avg: 0.9818\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate\n",
        "all_metrics, avg_metrics = train_and_evaluate(df, label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgL1iowaDev_",
        "outputId": "caf9e70a-1166-4b8f-b52a-2206b698f315"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'analyzing_surroundings': {'precision': 1.0,\n",
              "   'recall': 1.0,\n",
              "   'f1-score': 1.0,\n",
              "   'support': 425.0},\n",
              "  'asking_for_direction': {'precision': 0.9690909090909091,\n",
              "   'recall': 0.970856102003643,\n",
              "   'f1-score': 0.9699727024567789,\n",
              "   'support': 549.0},\n",
              "  'service_recommendation': {'precision': 0.967741935483871,\n",
              "   'recall': 0.96579476861167,\n",
              "   'f1-score': 0.9667673716012085,\n",
              "   'support': 497.0},\n",
              "  'accuracy': 0.9775662814411965,\n",
              "  'macro avg': {'precision': 0.9789442815249267,\n",
              "   'recall': 0.9788836235384376,\n",
              "   'f1-score': 0.9789133580193291,\n",
              "   'support': 1471.0},\n",
              "  'weighted avg': {'precision': 0.9775653643959163,\n",
              "   'recall': 0.9775662814411965,\n",
              "   'f1-score': 0.9775651919337677,\n",
              "   'support': 1471.0}},\n",
              " {'analyzing_surroundings': {'precision': 1.0,\n",
              "   'recall': 0.9976470588235294,\n",
              "   'f1-score': 0.9988221436984688,\n",
              "   'support': 425.0},\n",
              "  'asking_for_direction': {'precision': 0.9871794871794872,\n",
              "   'recall': 0.9817850637522769,\n",
              "   'f1-score': 0.9844748858447488,\n",
              "   'support': 549.0},\n",
              "  'service_recommendation': {'precision': 0.9780439121756487,\n",
              "   'recall': 0.9859154929577465,\n",
              "   'f1-score': 0.9819639278557114,\n",
              "   'support': 497.0},\n",
              "  'accuracy': 0.9877634262406526,\n",
              "  'macro avg': {'precision': 0.9884077997850453,\n",
              "   'recall': 0.988449205177851,\n",
              "   'f1-score': 0.9884203191329762,\n",
              "   'support': 1471.0},\n",
              "  'weighted avg': {'precision': 0.9877969835573324,\n",
              "   'recall': 0.9877634262406526,\n",
              "   'f1-score': 0.9877717168898061,\n",
              "   'support': 1471.0}},\n",
              " {'analyzing_surroundings': {'precision': 0.9976470588235294,\n",
              "   'recall': 0.9976470588235294,\n",
              "   'f1-score': 0.9976470588235294,\n",
              "   'support': 425.0},\n",
              "  'asking_for_direction': {'precision': 0.9694793536804309,\n",
              "   'recall': 0.9836065573770492,\n",
              "   'f1-score': 0.976491862567812,\n",
              "   'support': 549.0},\n",
              "  'service_recommendation': {'precision': 0.9795501022494888,\n",
              "   'recall': 0.9637826961770624,\n",
              "   'f1-score': 0.9716024340770791,\n",
              "   'support': 497.0},\n",
              "  'accuracy': 0.9809653297076818,\n",
              "  'macro avg': {'precision': 0.9822255049178162,\n",
              "   'recall': 0.9816787707925471,\n",
              "   'f1-score': 0.9819137851561401,\n",
              "   'support': 1471.0},\n",
              "  'weighted avg': {'precision': 0.9810200992444272,\n",
              "   'recall': 0.9809653297076818,\n",
              "   'f1-score': 0.9809520341849334,\n",
              "   'support': 1471.0}},\n",
              " {'analyzing_surroundings': {'precision': 1.0,\n",
              "   'recall': 1.0,\n",
              "   'f1-score': 1.0,\n",
              "   'support': 425.0},\n",
              "  'asking_for_direction': {'precision': 0.9746835443037974,\n",
              "   'recall': 0.9817850637522769,\n",
              "   'f1-score': 0.9782214156079855,\n",
              "   'support': 549.0},\n",
              "  'service_recommendation': {'precision': 0.9797160243407708,\n",
              "   'recall': 0.971830985915493,\n",
              "   'f1-score': 0.9757575757575757,\n",
              "   'support': 497.0},\n",
              "  'accuracy': 0.9836845683208701,\n",
              "  'macro avg': {'precision': 0.9847998562148561,\n",
              "   'recall': 0.9845386832225899,\n",
              "   'f1-score': 0.9846596637885203,\n",
              "   'support': 1471.0},\n",
              "  'weighted avg': {'precision': 0.9836982528349068,\n",
              "   'recall': 0.9836845683208701,\n",
              "   'f1-score': 0.9836812184366411,\n",
              "   'support': 1471.0}},\n",
              " {'analyzing_surroundings': {'precision': 1.0,\n",
              "   'recall': 1.0,\n",
              "   'f1-score': 1.0,\n",
              "   'support': 424.0},\n",
              "  'asking_for_direction': {'precision': 0.9675090252707581,\n",
              "   'recall': 0.97632058287796,\n",
              "   'f1-score': 0.971894832275612,\n",
              "   'support': 549.0},\n",
              "  'service_recommendation': {'precision': 0.9735772357723578,\n",
              "   'recall': 0.9637826961770624,\n",
              "   'f1-score': 0.9686552072800809,\n",
              "   'support': 497.0},\n",
              "  'accuracy': 0.9789115646258504,\n",
              "  'macro avg': {'precision': 0.980362087014372,\n",
              "   'recall': 0.9800344263516741,\n",
              "   'f1-score': 0.9801833465185643,\n",
              "   'support': 1470.0},\n",
              "  'weighted avg': {'precision': 0.9789322047976244,\n",
              "   'recall': 0.9789115646258504,\n",
              "   'f1-score': 0.9789060550595314,\n",
              "   'support': 1470.0}}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ioNtkT2t0My",
        "outputId": "bbae43f0-75de-45ff-f0ce-002743645735"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9817782340672503,\n",
              " 'macro avg': 0.9828180945231061,\n",
              " 'weighted avg': 0.981775243300936}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "def load_model_for_prediction(model_name=\"distilbert-base-uncased\",\n",
        "                            num_classes=None,\n",
        "                            weights_path=None):\n",
        "    \"\"\"\n",
        "    Load the trained model with weights\n",
        "    \"\"\"\n",
        "    model = DistillbertIntentModel(model_name, num_classes)\n",
        "\n",
        "    # Create a dummy input to build the model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    dummy_input = tokenizer(\"dummy text\", return_tensors=\"tf\", padding=True)\n",
        "    _ = model(dummy_input)  # This builds the model\n",
        "\n",
        "    # Load the weights\n",
        "    if weights_path:\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict_intent(text, model, label_encoder, model_name=\"distilbert-base-uncased\", max_length=128):\n",
        "    \"\"\"\n",
        "    Make predictions using the trained model\n",
        "\n",
        "    Args:\n",
        "        text: String or list of strings to predict\n",
        "        model: Trained DistillbertIntentModel instance\n",
        "        label_encoder: The LabelEncoder used during training\n",
        "        model_name: Name of the BERT model used during training\n",
        "        max_length: Maximum sequence length used during training\n",
        "\n",
        "    Returns:\n",
        "        Predicted labels and probabilities\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Handle both single string and list of strings\n",
        "    if isinstance(text, str):\n",
        "        text = [text]\n",
        "\n",
        "    # Tokenize\n",
        "    encodings = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model(encodings, training=False)\n",
        "\n",
        "    # Get probabilities\n",
        "    probabilities = tf.nn.softmax(predictions, axis=-1)\n",
        "\n",
        "    # Get predicted labels\n",
        "    pred_indices = tf.argmax(probabilities, axis=-1).numpy()\n",
        "    pred_labels = label_encoder.inverse_transform(pred_indices)\n",
        "\n",
        "    return pred_labels, probabilities.numpy()"
      ],
      "metadata": {
        "id": "MU2rspR-vj58"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First load the model with weights from a specific fold\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = load_model_for_prediction(\n",
        "    model_name=\"distilbert-base-uncased\",\n",
        "    num_classes=num_classes,\n",
        "    weights_path='/content/best_model_fold_1'  # or whichever fold performed best\n",
        ")\n",
        "\n",
        "# 2. Make predictions\n",
        "texts = [\n",
        "    \"can you guide me to Blok M?\",\n",
        "    \"Where is the nearest escalator\",\n",
        "    \"can you recommend ramen places\"\n",
        "]\n",
        "\n",
        "predicted_labels, probabilities = predict_intent(\n",
        "    texts,\n",
        "    model,\n",
        "    label_encoder,\n",
        "    model_name=\"distilbert-base-uncased\"\n",
        ")\n",
        "\n",
        "# 3. Print results\n",
        "for text, label, probs in zip(texts, predicted_labels, probabilities):\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Predicted Intent: {label}\")\n",
        "    print(f\"Confidence: {np.max(probs):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZe86l__t1D8",
        "outputId": "7edde5cf-924e-4bf2-d803-ce5cdf7b4312"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: can you guide me to Blok M?\n",
            "Predicted Intent: asking_for_direction\n",
            "Confidence: 0.9863\n",
            "\n",
            "Text: Where is the nearest escalator\n",
            "Predicted Intent: analyzing_surroundings\n",
            "Confidence: 0.9505\n",
            "\n",
            "Text: can you recommend ramen places\n",
            "Predicted Intent: service_recommendation\n",
            "Confidence: 0.9980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "835pZlqR9Xr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}